# References

_Seed sources are prioritized; adjacent works included sparingly._

- **[S1]** What Shapes Neural Networks? A Mechanistic Understanding of Handcrafted Inductive Biases (2023). arXiv preprint. https://arxiv.org/pdf/2305.19730 — Discusses inductive biases; includes curvature/flattening perspectives relevant for representation geometry.
- **[S2]** Geometry Linked to Untangling Efficiency Reveals Structure and Computation in Neural Populations (2024/2025). bioRxiv preprint. https://www.biorxiv.org/content/10.1101/2024.02.26.582157v1.full — Introduces simulation- and mean-field capacity (α_sim, α_mf) and anchor/axis measures; GLUE.
- **[S3]** How Deep Nets Flatten and Linearize Representations (2016). arXiv preprint. https://arxiv.org/pdf/1602.04723 — Early formalization of layer-wise linearization/flattening behaviors.
- **[S4]** From Louvain to Leiden: guaranteeing well-connected communities (2019). arXiv / Scientific Reports. https://arxiv.org/abs/1810.08473 — Leiden guarantees connected communities; Louvain may produce disconnected ones.
- **[S5]** Tweet by Peyman Milanfar on manifold capacity & geometry (motivational) (2024). Tweet. https://x.com/docmilanfar/status/1988842744073048553 — Motivational/weak evidence; used only to motivate questions.
- **[S6]** Functions are Vectors (2018). Blog. https://thenumb.at/Functions-are-Vectors/ — Accessible explanation of function spaces and Hilbert spaces; used as pedagogical bridge.
- **[S7]** Representation Learning via Manifold Flattening and Reconstruction (2024). JMLR. https://www.jmlr.org/papers/volume25/23-0615/23-0615.pdf — Pairs flattening with reconstruction to preserve structure; curvature shaping.
- **[S8]** Approximating Latent Manifolds via Vanishing Ideals (2025). arXiv preprint. https://arxiv.org/pdf/2502.15051 — Algebraic manifolds and vanishing ideals as model for latent geometry.
- **[S9]** Geometric Deep Learning: going beyond Euclidean data (2017). arXiv preprint. https://arxiv.org/pdf/1611.08097 — Unifies manifold/graph/symmetry views; background for graphs and manifolds.
- **[S10]** Domínguez-Olmedo et al., PMLR V202 (2023) (2023). PMLR. https://proceedings.mlr.press/v202/dominguez-olmedo23a/dominguez-olmedo23a.pdf — Adjacent method touching manifold distances/structure (referenced sparingly).
- **[A1]** Separability and geometry of object manifolds in deep neural networks (2020). Nature Communications. https://www.nature.com/articles/s41467-020-14578-5 — Core MFTMA definitions: capacity α=P/N, anchor points, effective radius R_M and dimension D_M; empirical layerwise trends.
- **[A2]** Learning Efficient Coding of Natural Images with Maximum Manifold Capacity Representations (2023). arXiv preprint. https://ar5iv.org/html/2303.03307 — MMCR objective; inverse capacity α^{-1} = E[F(T)] support-function/KKT formulation; anchor interpretation.