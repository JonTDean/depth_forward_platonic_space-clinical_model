# ğŸ—‚ï¸ Kanban â€” *manifoldâ€‘clinical_ontology* (vectorized ontologies + capacity)

> Scope: build, measure, and govern the **manifold geometry** of NCIt/OBO concept embeddings across the audience bundles you listed (Academic, Clinical, Data, Doctoral, Engineering, Leadership, Linguist, Mathematician, Philosopher, Physicist, Projectâ€‘Design, Research, UX/UI).

**Columns** use GitHubâ€‘style checkboxes and IDs (TERM/GEO/MAP/GRAPH/EVAL) you can lift into issues.

### Backlog

* [ ] **TERMâ€‘01 | Vocabulary curation plan** â€” nominate concept slices (oncology imaging, labs, histology) and sources (NCIt/OBO, UMLS). Output: `/docs/reference-terminology/manifold-clinical_ontology/domain/*_Bundle.md` checklists per bundle.
* [ ] **TERMâ€‘02 | Synonym gate** â€” define allowâ€‘list/banâ€‘list and provenance fields; wire to â€œsynonym gateâ€ in docs. Output: policy YAML + code hooks.
* [ ] **GEOâ€‘01 | Capacity metrics** â€” implement perâ€‘concept **R_M**, **D_M (PR)**, centroid correlations **Ï_CC**, **Î±_mf** and **Î±_sim** + bootstrap CIs; JSON logging. (Specs in Report Â§IVâ€‘A.) ([PMC][1])
* [ ] **GEOâ€‘02 | Hierarchyâ€‘aware flattening** â€” prototype centroid lowâ€‘rank projection + withinâ€‘class whitening; target â†“R_M, â†“D_M, â†“Ï_CC without degrading mapping. (Report Â§IVâ€‘B.) ([PMC][1])
* [ ] **GRAPHâ€‘01 | Leiden cleaning** â€” preâ€‘embedding community detection with Leiden; reject badly connected partitions; Î³â€‘sweep and stability curve. (Report Â§VI.)
* [ ] **GRAPHâ€‘02 | Resolution policy** â€” codify Î³ defaults per ontology slice (fine vs coarse granularity) + regression tests.
* [ ] **MAPâ€‘01 | Reâ€‘ranker hooks** â€” expose geometry flags (`capacity_low`, `centroid_corr_high`) in `MappingResult.reason`; connect to Auto/Review/NoMatch thresholds. (Docs already show state machines.)
* [ ] **EVALâ€‘01 | Counterexamples** â€” add generators where **capacity stays constant but mapping accuracy changes** (and vice versa). (Report Â§V, Â§VII.)
* [ ] **ALGâ€‘01 | Algebraic probes** â€” implement lowâ€‘degree vanishingâ€‘ideal detector for collapsed manifolds; alert when crossâ€‘class residual separation < Îµ. (Report Â§IVâ€‘D.)
* [ ] **OPSâ€‘01 | Dashboards & SLAs** â€” drift alerts when |Î±_simâˆ’Î±_mf|/Î±_sim > 0.2, Ï_CC â†‘ > 0.05 abs, or Mean(R_MâˆšD_M) â†‘ > 20% vs. baseline. (Report Â§V, Â§VIII.) ([PMC][1])

### Doing

* [ ] **GEOâ€‘01â€‘A | DFPS capacity evaluator** â€” add `geometry_probe()` producing `(R_M, D_M, Ï_CC, Î±_mf, Î±_sim)`; integrate with `EvalSummary` and pipeline reports. (Wire to `lib/domain/eval` and `lib/domain/mapping`.)
* [ ] **GRAPHâ€‘01â€‘A | Leiden in pipeline** â€” CLI to run Î³â€‘grid, fail build on disconnected/badly connected communities â‰  0.

### Review

* [ ] **TERMâ€‘02â€‘A | Synonym gate A/B** â€” show effect on R_M, D_M, Ï_CC and mapping F1 before/after; rollback automatically if Mean(R_MâˆšD_M) â†‘ > 20%.
* [ ] **ALGâ€‘01â€‘A | Vanishingâ€‘ideal alerts** â€” validate against synthetic collapsed classes and real NCIt slices.

### Done

* [ ] **Docs skeletons** â€” audience bundle pages + FHIR/NCIt system design (provided).

---

# A Consortiumâ€‘ofâ€‘Minds Inquiry into Manifold Capacity, Geometry, and Vectorized Ontologies

## I. Executive Summary (â‰¤300 words)

We connect **manifold capacity theory** to **vectorized ontologies** (NCIt/OBO) and deliver DFPSâ€‘ready methods to **measure**, **shape**, and **govern** representation geometry. Capacity ( \alpha ) is the critical load (P/N) enabling linear separability of (P) manifolds in (N) dimensions. Meanâ€‘field theory expresses ( \alpha^{-1} ) as an expectation of a supportâ€‘function optimization; **anchor points** induced by KKT conditions define **effective radius** (R_M) and **effective dimension** (D_M); the combined scale (R_M\sqrt{D_M}) (width) limits capacity. Empirical and theoretical work show layerwise improvements in separability track decreases in (D_M), (R_M), and centroid correlations ( \rho_{CC} ). We operationalize this with:
**(A)** capacity estimators ((R_M, D_M, \rho_{CC}, \alpha_{mf}, \alpha_{sim})) with bootstrap CIs and JSON logs;
**(B)** hierarchyâ€‘aware flattening (centroid lowâ€‘rank removal + withinâ€‘class whitening; MMCRâ€‘style objective) to reduce (R_M, D_M, \rho_{CC});
**(C)** **Leiden** community detection to guarantee wellâ€‘connected ontology graph communities preâ€‘embedding; and
**(D)** an evaluation harness with **counterexamples** and **algebraic probes** (vanishing ideals) to catch collapsed class manifolds.
These mechanisms plug directly into DFPS crates (`dfps_mapping`, `dfps_eval`, `dfps_pipeline`, `dfps_terminology`) and enforce safety with alerts when (|\alpha_{sim}-\alpha_{mf}|/\alpha_{sim} > 0.2), ( \rho_{CC}) spikes, or (R_M\sqrt{D_M}) inflates. ([PMC][1])

---

## II. Claims Matrix

| Paper                       | Claim (short)                                                                                                                               | Formal statement / locus                                                                                                       | Evidence                                                           | Relation                                          |
| --------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------ | ------------------------------------------------- |
| Dapello etâ€¯al. 2021 (MFTMA) | Capacity via meanâ€‘field; anchors define (R_M, D_M); width (R_M\sqrt{D_M}) controls (\alpha).                                                | Inverse capacity ( \alpha^{-1} = \mathbb{E}_T[F(T)]); anchors from KKT; (R_M, D_M) from anchor statistics; ball approximation. | arXiv PDF (defs & derivations). Strong.                            | Extends earlier theory; basis for our estimators. |
| Yerxa etâ€¯al. 2023 (MMCR)    | Loss aligns with capacity; supportâ€‘function/KKT framing of (\alpha).                                                                        | Presents ( \alpha^{-1} ) via support function; MMCR objective (L \approx -|GZ|_*) aligns embeddings.                           | PMC article; equations & proof sketches. Mediumâ€‘strong. ([PMC][1]) | Extends: suggests capacityâ€‘aware training.        |
| Chou etâ€¯al. 2025 (GLUE)     | Dual estimators (\alpha_{sim}) & (\alpha_{mf}); anchor/axis metrics; empirical validation.                                                  | Methods & Supplement S1 list geometric measures and capacity constructs, contrasting (\alpha_{sim}) vs (\alpha_{mf}).          | PMC full text. Medium. ([PMC][1])                                  | Extends: practical probes & gap analysis.         |
| Traag etâ€¯al. 2019 (Leiden)  | Leiden yields **connected** communities; converges to partitions without badly connected communities; Louvain can output disconnected ones. | Theory + experiments; guarantees after iterations.                                                                             | arXiv (ar5iv) article. Strong.                                     | Contradicts reliance on Louvain for graph health. |
| Bartlett (RKHS notes)       | Functionsâ€‘asâ€‘vectors via RKHS; evaluation functionals bounded; kernel view unifies feature maps.                                            | Lecture notes defining RKHS and kernels; function space Hilbert structure.                                                     | Berkeley notes. Medium (tutorial). ([People @ EECS][2])            |                                                   |

---

## III. Formal Notes (definitions, lemmas, sketches)

**Definition 1 (Capacity).** For (P) manifolds ({\mathcal{M}*\mu}*{\mu=1}^P \subset \mathbb{R}^N) with random labels (y_\mu\in{\pm1}), the **linear classification capacity** is (\alpha_c = P_c/N), where (P_c) is the largest (P) such that a separating hyperplane exists w.h.p. **Meanâ€‘field** inverse capacity:
[
\alpha^{-1}=\mathbb{E}*{T}!\left[\min*{V}|V-T|*2^2\ \text{s.t.}\ \min*{S\in \mathrm{conv}(\mathcal{M})} V!\cdot! S \ge 0\right],
]
with **anchors** (\tilde S(T)\in \mathrm{conv}(\mathcal{M}_\mu)) emerging from KKT conditions. ([PMC][1])

**Definition 2 (Effective radius/dimension).** Let anchor statistics define: (R_M:=\mathbb{E}|\tilde S|). Let (C) be the covariance of anchorâ€‘projections; **effective dimension** (D_M := \frac{(\mathrm{tr},C)^2}{\mathrm{tr}(C^2)}) (participation ratio). **Width** (W_M:=R_M\sqrt{D_M}). Capacity decreases monotonically with (W_M) under ball approximation.

**Definition 3 (Centroid correlations).** With centroids (c_\mu = \mathbb{E}*{x\in\mathcal{M}*\mu}[x]), define (\rho_{CC} = \mathbb{E}*{\mu\ne\nu}[\cos(c*\mu, c_\nu)]). Lowâ€‘rank shared components in ({c_\mu}) reduce capacity; projecting out this subspace increases effective (\alpha).

**Lemma 1 (Ball approximation).** For ensembles with random centers/orientations, replacing each (\mathcal{M}_\mu) by a (D_M)â€‘ball of radius (R_M) preserves (\alpha) to first order; thus (W_M) controls separability. *Sketch:* match firstâ€‘ and secondâ€‘order anchor statistics; invoke meanâ€‘field selfâ€‘consistency.

**Definition 4 (Extrinsic vs. intrinsic curvature).** Intrinsic curvature derives from the manifoldâ€™s Riemannian metric; extrinsic curvature from the second fundamental form of its embedding in (\mathbb{R}^N). Flattening seeks transforms that reduce extrinsic curvature (and thus (D_M)) without destroying class topology. (Operationalized in Â§IVâ€‘B.) ([PMC][1])

**Definition 5 (Functionsâ€‘asâ€‘vectors, RKHS).** A Reproducing Kernel Hilbert Space (RKHS) (\mathcal{H}) embeds functions as vectors with inner product induced by a positiveâ€‘definite kernel (k). Embeddings via feature maps (\phi(x)) realize geometric analysis in (\mathcal{H}). ([People @ EECS][2])

**Proposition 1 (Correlation projection).** If ({c_\mu}) have rankâ€‘(K\ll P) structure, projecting (x\mapsto x - UU^\top x) onto the orthogonal complement of topâ€‘(K) centroid PCs leaves withinâ€‘manifold geometry unchanged while increasing (\alpha). *Sketch:* Remove shared bias limiting margin; meanâ€‘field capacity depends on projected centers.

---

## IV. Algorithms (pseudocode, complexity, expected effect)

### A) Capacity estimation from embeddings (DFPS evaluator)

**Goal.** Estimate (R_M, D_M, \rho_{CC}, \alpha_{mf}, \alpha_{sim}) per concept and globally; emit JSON.

**Pseudocode (vector/matrix primitives):**

```python
def capacity_metrics(X: np.ndarray, y: np.ndarray, m_list=[32,128,512], B=1000):
    # X: (N,D), y: concept IDs
    C = np.unique(y)
    mu = {c: X[y==c].mean(0) for c in C}
    # within-class covariance eigenspectrum
    eig = {}
    R = {}
    for c in C:
        Xc = X[y==c]; Z = Xc - mu[c]
        # covariance via thin SVD
        U, s, _ = np.linalg.svd(Z/np.sqrt(max(1,len(Z)-1)), full_matrices=False)
        lam = s**2; eig[c] = lam
        PR = (lam.sum()**2) / (lam**2).sum()
        R[c] = np.sqrt((Z**2).sum(1).mean())
    # centroid-correlation matrix (sparse kNN optional)
    rho = centroid_cosines(list(mu.values()))
    # Î±_mf via ball approximation
    Rbar = np.mean(list(R.values())); Dbar = np.median([pr(eig[c]) for c in C])
    alpha_mf = alpha_ball(Rbar, Dbar)        # lookup/closed form
    # Î±_sim curve: project to m dims then linear OVR classifier
    alpha_sim = {}
    for m in m_list:
        Xproj = pca_or_rproj(X, m)
        alpha_sim[m] = one_vs_rest_linear_acc(Xproj, y)    # balanced Acc/F1
    # bootstrap CIs for Rbar, Dbar, rho
    CIs = bootstrap_metrics(...)
    return dump_json(mu, R, eig, rho, alpha_mf, alpha_sim, CIs)
```

**Cost.** Per class SVD on ((n_c\times D)) (use truncated when (D) large): (O(\min(n_c D^2, D n_c^2))). Linear classifiers: (O(\sum_m N m)).
**Expected effect.** Interventions that **lower (R_M, D_M, \rho_{CC})** increase (\alpha); deviations between (\alpha_{sim}) and (\alpha_{mf}) indicate meanâ€‘field mismatch. ([PMC][1])

---

### B) Hierarchyâ€‘aware flattening / linearization

**Idea.** Emulate known layerwise effects: reduce (D_M) (pool/aggregate), reduce (R_M) (nonlinearity + normalization), reduce (\rho_{CC}) (remove shared modes).

**Recipe.**

1. **Centroid projection:** (x \leftarrow x - U_KU_K^\top x) where (U_K) spans topâ€‘(K) centroid PCs.
2. **Whiten withinâ€‘class:** (x \leftarrow \Sigma_c^{-1/2}(x-\mu_c)) with shrinkage.
3. **MMCRâ€‘style loss:** train a small adapter (g_\theta) to minimize (L=-|GZ|** + \lambda\cdot \text{topology_penalty}) (preserve nearestâ€‘centroid ordering). Expect (R_M\downarrow, D_M\downarrow, \rho*{CC}\downarrow). ([PMC][1])

---

### C) Graphâ€‘aware embedding updates (vectorized ontology layer)

**Inputs.** Ontology graph (G=(V,E)) with synonym/ancestor edges; text/definition corpora.
**Algorithm.**

* Build concept samples by **synonym sets** and **definition paraphrases**; gate lowâ€‘quality synonyms (provenance + overlap tests).
* Add **graph context pooling**: (x_c \leftarrow \mathrm{Agg}{x_c, x_{\text{syn}(c)}, x_{\text{anc}(c)}}) (mean/max attention with learned weights).
* Recompute geometry; rollback if Mean((R_M\sqrt{D_M})) â†‘ > 20% or (\rho_{CC}) â†‘ > 0.05 absolute.

---

### D) Capacityâ€‘preserving community detection (Graph Health)

**Pipeline.**

1. Run **Leiden** (modularity or CPM) over (G) with Î³ grid.
2. **Reject** partitions with any **disconnected** or **badly connected** communities; iterate until guarantee holds; store Î³â†’partition curve.
3. Use communities for negative sampling / curriculum, then embed.
   **Rationale.** Leiden guarantees connected communities after iterations; Louvain may return disconnected ones, distorting geometry estimates.

---

### E) Algebraic probes (vanishing ideals; highâ€‘dim approximation)

**Goal.** Detect **collapsed** class manifolds (algebraic overlap) beyond metric cues.
**Approximation.** Fit lowâ€‘degree polynomial features (\phi_d(x)) with Lasso to find nonâ€‘trivial (p(x)) that nearly vanish on class (c): minimize (|p(\phi_d(X_c))|_2^2 + \lambda|p|_1) while maximizing residual on negatives. Alert when many classes share the same vanishing polynomials or when classâ€‘specific vanishing error (\ll) offâ€‘class error margin.

---

## V. Experiments (design matrix, metrics, thresholds)

**Design factors.**

* **Geometry:** (R_M \in {0.1,0.2,0.4}), (D_M \in {4,8,16,32}), (\rho_{CC}\in{0,0.05,0.1,0.2}).
* **Curvature:** extrinsic curvature parameter (\kappa\in{0,\kappa_{\text{mild}},\kappa_{\text{high}}}) via curved synthetic manifolds (swissâ€‘roll â†’ linear map â†’ noise).
* **Graph:** Louvain vs Leiden; Î³ sweep.
* **Interventions:** centroid projection on/off; whitening on/off; MMCR adapter on/off.

**Datasets.** (a) NCIt slices (imaging, histology); (b) synthetic manifolds matched to empirical (R_M, D_M); (c) graph variants pre/post Leiden cleaning.

**Metrics.**

* Geometry: (R_M, D_M, \rho_{CC}, W_M).
* Capacity: (\alpha_{mf}) (ball approx), (\alpha_{sim}(m)) curve via projected linear OVR accuracy.
* Downstream: mapping **F1 / AutoMapped / NeedsReview rates**.

**Acceptance thresholds (gate deployments).**

* (|\alpha_{sim}-\alpha_{mf}|/\alpha_{sim} \le 0.2) (else investigate model assumptions).
* **No** increase in Mean (\rho_{CC}) > 0.05 abs; **no** increase in Mean (W_M) > 20% without compensating (\alpha_{sim})â†‘ â‰¥ 10%.
* **Leiden** partition selected when disconnected fraction = 0 and stability across Î³ is high. ([PMC][1])

**Counterexamples (implement in eval).**

1. **Same (\alpha), worse mapping:** hold (R_M, D_M) fixed; rotate a subset of centroids to increase nearestâ€‘centroid confusability â†’ mapping F1â†“ at similar (\alpha_{mf}).
2. **Similar (\alpha), better mapping:** project out topâ€‘K centroid modes (reduce (\rho_{CC})), add mild withinâ€‘class noise to keep (W_M) â‰ˆ const â†’ accuracyâ†‘.

**Statistical testing.** 1,000 bootstraps per run; report 95% CIs; permutation tests for classâ€‘label invariance. ([PMC][1])

---

## VI. Graph / Ontology Actions

1. **Prefer Leiden** to guarantee connected communities and iterative convergence to partitions without badly connected subsets. Store Î³â†’cluster count & stability; reject any partition failing the connectivity guarantee.
2. **Prune** dangling/lowâ€‘degree nodes; **collapse** nearâ€‘duplicate synonym cliques (cosine > Ï„) postâ€‘Leiden.
3. **Hierarchy handling:** ancestorâ€‘aware pooling with decays (e.g., (w_{\text{ancestor}}=\lambda^{\text{depth}})); reâ€‘measure (R_M, D_M, \rho_{CC}) and roll back if thresholds fail.

---

## VII. Risks & Redâ€‘Team Findings (with mitigations)

* **Meanâ€‘field mismatch.** Nonâ€‘Gaussian centers or structured labels break (\alpha_{mf}). *Mitigation:* compare to (\alpha_{sim}); alert if gap > 0.2; inspect lowâ€‘rank center structure; project it out. ([PMC][1])
* **Synonym noise.** Inflates (R_M) and can raise (\rho_{CC}). *Mitigation:* synonym gate + revert if Mean (W_M) â†‘ > 20%.
* **Community artifacts.** Louvain fragmentation distorts sampling and geometry. *Mitigation:* Leiden + connectivity tests in CI.
* **Collapsed manifolds (algebraic).** Metric probes miss it. *Mitigation:* vanishingâ€‘ideal detector (lowâ€‘degree) to flag shared polynomials across classes.
* **Overâ€‘flattening.** Excess whitening can erase clinically meaningful variation. *Mitigation:* topology penalty (nearestâ€‘centroid order preservation) and perâ€‘bundle clinical review.

---

## VIII. Roadmap & Integration (DFPS crates/docs; metrics; CI)

**30 days (A & C).**

* Implement `geometry_probe()` (Rust) producing `{r_m, d_m, rho_cc, alpha_mf, alpha_sim}`; attach to `MappingResult.reason` flags.
* Add **Leiden** pipeline (Î³ grid + connectivity checks) and fail CI on bad partitions.
* **Logs/JSON**: global + perâ€‘class metrics with bootstrap CIs; dashboards plot (W_M), (\rho_{CC}), (\alpha_{sim}(m)).

**60 days (A2 & B).**

* Ship **flattening adapter** (centroid projection + whitening + MMCRâ€‘style loss) as optional preâ€‘ranker stage; A/B on NCIt slices.
* Wire **counterexample** generators into `dfps_eval` to validate gating rules. ([PMC][1])

**90 days (D).**

* **Algebraic probes** (vanishingâ€‘ideal alarms) in `dfps_eval`; policy to block deployments on collapse alerts.

**Crate touchâ€‘points.**

* `lib/domain/mapping` â€” add `geometry_probe()` & flags; flattening preâ€‘ranker hook.
* `lib/domain/eval` â€” metrics structs + bootstrap; counterexample generators.
* `lib/domain/pipeline` â€” surface metrics to observability.
* `lib/domain/terminology` â€” synonym gate + provenance; Leiden outputs for graph health.

**CI checks (defaults).**

* `|Î±_simâˆ’Î±_mf|/Î±_sim â‰¤ 0.2`, `Mean Ï_CC â‰¤ baseline+0.05`, `Mean(W_M) â‰¤ baselineÃ—1.2`, **Leiden connectivity = pass**. ([PMC][1])

---

## IX. References (seedâ€‘biased; URLs via citations)

* Dapello, J. etâ€¯al. *Neural population geometry reveals the role of stochasticity in robust perception.* (MFTMA; anchors, (R_M, D_M), width). arXiv PDF.
* Yerxa, T. E. etâ€¯al. *Learning Efficient Coding of Natural Images with Maximum Manifold Capacity Representations.* (Supportâ€‘function capacity; MMCR objective.) PMC article. ([PMC][1])
* Chou, C.â€‘N. etâ€¯al. *Geometry Linked to Untangling Efficiency Reveals Structure and Computation in Neural Populations.* (Dual (\alpha_{sim})/(\alpha_{mf}); geometry metrics; empirics.) PMC article. ([PMC][1])
* Traag, V. A. etâ€¯al. *From Louvain to Leiden: guaranteeing wellâ€‘connected communities.* (Connectivity guarantees; convergence.) arXiv (ar5iv) article.
* Bartlett, P. *Reproducing Kernel Hilbert Spaces.* (Functionsâ€‘asâ€‘vectors; RKHS.) Lecture notes. ([People @ EECS][2])
* **Vanishing ideals**: *Approximating Latent Manifolds in Neural Networks via Vanishing Ideals.* (Algebraic probes; practical approximations.) arXiv PDF.

---

### Appendixâ€”DFPS Capacity Estimation Plan (concise cutâ€‘sheet)

* **Inputs:** embeddings (X\in\mathbb{R}^{N\times D}), labels (concept IDs).
* **Compute:** perâ€‘class ( \mu_c, \Sigma_c\Rightarrow R_c, PR_c\ (D_{M,c}),) pairwise centroid cosines; global (R_M=\mathrm{mean},R_c), (D_M=\mathrm{median},PR_c), ( \rho_{CC}=\mathrm{mean,cos}).
* **Capacity:** ( \alpha_{mf}=\alpha_{\text{ball}}(R_M,D_M)); ( \alpha_{sim}(m)) from projected linear separability curves.
* **CIs:** bootstrap perâ€‘class and global metrics (B=1000).
* **Emit JSON:** keyed by concept + global summary; wire to `dfps_eval` and dashboards. ([PMC][1])

### Appendixâ€”Graph Community Tuning (reproducible steps)

1. Run Leiden with Î³âˆˆÎ“; iterate until **no badly connected** communities; store Î³â†’partition + metrics.
2. Fail CI if any disconnected community exists; pin Î³ that maximizes stability and downstream ( \alpha_{sim}).

### Appendixâ€”Counterexample Construction (generators)

* **Hold (R_M, D_M), vary (\rho_{CC}):** simulate equalâ€‘covariance Gaussians; rotate centroids to reduce interâ€‘class angles â†’ F1â†“ at constant (\alpha_{mf}).
* **Reduce (\rho_{CC}), keep (W_M)â‰ˆconst:** project out topâ€‘K centroid PCs; add isotropic noise to keep (R_M) stable â†’ F1â†‘ with (\alpha_{mf}) unchanged.

### Appendixâ€”Algebraic Probes (tractable approximation)

* Build degreeâ€‘(d) polynomial features; L1â€‘regularized regression to find lowâ€‘norm (p(x)) s.t. (p(x)\approx 0) on class, (> \epsilon) offâ€‘class; alert on shared vanishing polynomials across classes.
